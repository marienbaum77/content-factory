========================================
ROOT FILES
========================================


========================================
FILE: requirements.txt
========================================
aiokafka==0.12.0
aiogram==3.4.1
pydantic-settings==2.2.1
sqlalchemy==2.0.29
asyncpg==0.29.0
feedparser==6.0.10
trafilatura==1.6.3
httpx==0.27.0
lxml==5.1.0

========================================
FILE: docker-compose.yml
========================================
services:
  # === –ò–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–ê ===
  kafka:
    image: dockerhub.timeweb.cloud/bitnami/kafka:3.6.1
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,INTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092,INTERNAL://kafka:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka-ui:
    image: dockerhub.timeweb.cloud/provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
    depends_on:
      - kafka

  postgres:
    image: dockerhub.timeweb.cloud/library/postgres:15-alpine
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${DB_USER}
      - POSTGRES_PASSWORD=${DB_PASS}
      - POSTGRES_DB=${DB_NAME}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME}"]
      interval: 5s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dockerhub.timeweb.cloud/dpage/pgadmin4:8.5
    container_name: pgadmin
    ports:
      - "5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@admin.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    depends_on:
      - postgres

  # === –ù–ê–®–ò –ú–ò–ö–†–û–°–ï–†–í–ò–°–´ ===
  
  collector:
    build: .  # –°–æ–±–∏—Ä–∞–µ–º –∏–∑ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É—è Dockerfile)
    container_name: app-collector
    volumes:
      - ./src:/app/src
    command: python src/collector/main.py
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file: .env
    environment:
      # –ü–ï–†–ï–û–ü–†–ï–î–ï–õ–Ø–ï–ú –∞–¥—Ä–µ—Å–∞ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–µ—Ç–∏ Docker
      - KAFKA_BROKER=kafka:29092
      - DB_HOST=postgres

  processor:
    build: .
    container_name: app-processor
    volumes:
      - ./src:/app/src
    command: python src/processor/main.py
    depends_on:
      kafka:
        condition: service_healthy
    env_file: .env
    environment:
      - KAFKA_BROKER=kafka:29092

  publisher:
    build: .
    container_name: app-publisher
    volumes:
      - ./src:/app/src
    command: python src/publisher/main.py
    depends_on:
      kafka:
        condition: service_healthy
    env_file: .env
    environment:
      - KAFKA_BROKER=kafka:29092

volumes:
  kafka_data:
  postgres_data:

========================================
FILE: .env
========================================
# –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
DB_HOST=localhost
DB_PORT=5432
DB_USER=app_user
DB_PASS=app_password
DB_NAME=content_factory

# Kafka
KAFKA_BROKER=localhost:9092

# Telegram (–ë–ï–ó –∫–∞–≤—ã—á–µ–∫!)
TG_BOT_TOKEN=7878703369:AAHCIUK4Uq4CNQgUwbbIp651SopeToaN1kc
TG_CHAT_ID=@razvivashkins

OPENROUTER_API_KEY=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\n\n========================================
SRC DIRECTORY (Application Code)
========================================


========================================
FILE: src/publisher/main.py
========================================
import asyncio
import json
import logging
from aiokafka import AIOKafkaConsumer
from aiogram import Bot
from aiogram.enums import ParseMode
from config import settings

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

async def send_to_telegram(bot: Bot, data: dict):
    title = data.get("title", "No Title")
    summary = data.get("summary", "No Summary")
    link = data.get("original_url", "#")
    
    text = (
        f"<b>{title}</b>\n\n"
        f"{summary}\n\n"
        f"üëâ <a href='{link}'>–ß–∏—Ç–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª</a>"
    )
    
    try:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: settings.tg_chat_id (–±—ã–ª–æ telegram_chat_id)
        await bot.send_message(
            chat_id=settings.tg_chat_id, 
            text=text,
            parse_mode=ParseMode.HTML,
            disable_web_page_preview=True
        )
        logger.info(f"Sent to Telegram: {title}")
    except Exception as e:
        logger.error(f"Telegram error: {e}")

async def main():
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: settings.tg_bot_token (–±—ã–ª–æ telegram_bot_token)
    bot = Bot(token=settings.tg_bot_token)
    
    consumer = AIOKafkaConsumer(
        settings.topic_source,
        bootstrap_servers=settings.kafka_broker,
        group_id="telegram_publisher_group",
        value_deserializer=lambda x: json.loads(x.decode("utf-8"))
    )
    await consumer.start()
    logger.info("Publisher started. Waiting for content...")

    try:
        async for msg in consumer:
            data = msg.value
            logger.info(f"Received content: {data.get('title')}")
            await send_to_telegram(bot, data)
            
    finally:
        await consumer.stop()
        await bot.session.close()

if __name__ == "__main__":
    asyncio.run(main())

========================================
FILE: src/publisher/config.py
========================================
from typing import Union
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    kafka_broker: str
    topic_source: str = "content.ready"
    
    tg_bot_token: str
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: –†–∞–∑—Ä–µ—à–∞–µ–º –∏ —á–∏—Å–ª–æ (–¥–ª—è –ø—Ä–∏–≤–∞—Ç–Ω—ã—Ö), –∏ —Å—Ç—Ä–æ–∫—É (–¥–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö @channel)
    tg_chat_id: Union[int, str]

    model_config = SettingsConfigDict(
        env_file=".env", 
        env_file_encoding="utf-8",
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º DB_HOST, DB_USER –∏ –ø—Ä–æ—á–µ–µ,
        # —á—Ç–æ –ª–µ–∂–∏—Ç –≤ .env, –Ω–æ –Ω–µ –Ω—É–∂–Ω–æ –ü–∞–±–ª–∏—à–µ—Ä—É
        extra="ignore"
    )

settings = Settings()

========================================
FILE: src/collector/main.py
========================================
import asyncio
import json
import logging
import feedparser
from aiokafka import AIOKafkaProducer
from config import settings

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É –ë–î
from storage.db import init_db
from storage.repository import UrlRepository

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

async def get_kafka_producer():
    producer = AIOKafkaProducer(bootstrap_servers=settings.kafka_broker)
    await producer.start()
    return producer

async def main():
    logger.info("Service Collector started...")
    
    # 1. –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ –ë–î (–µ—Å–ª–∏ –µ—ë –Ω–µ—Ç)
    await init_db()
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
    repo = UrlRepository()
    
    producer = await get_kafka_producer()

    try:
        while True:
            logger.info(f"Fetching RSS from {settings.rss_url}")
            feed = feedparser.parse(settings.rss_url)

            for entry in feed.entries:
                link = entry.link
                
                # === –ü–†–û–í–ï–†–ö–ê –ù–ê –î–£–ë–õ–ò–ö–ê–¢–´ ===
                if await repo.is_processed(link):
                    logger.debug(f"Skipping {link} (already processed)")
                    continue
                # ==============================

                message = {
                    "title": entry.title,
                    "link": link,
                    "published": entry.published
                }

                value_json = json.dumps(message).encode('utf-8')
                await producer.send_and_wait(settings.kafka_topic, value_json)
                
                # === –ó–ê–ü–û–ú–ò–ù–ê–ï–ú –°–°–´–õ–ö–£ ===
                await repo.add(link, source="habr")
                logger.info(f"New article sent: {entry.title}")

            logger.info(f"Sleeping for {settings.poll_interval} seconds...")
            await asyncio.sleep(settings.poll_interval)

    except Exception as e:
        logger.error(f"Critical error: {e}")
    finally:
        await producer.stop()
        logger.info("Producer stopped")

if __name__ == "__main__":
    asyncio.run(main())

========================================
FILE: src/collector/__init__.py
========================================


========================================
FILE: src/collector/config.py
========================================
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    # === –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ ===
    # –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ Pydantic —Å–∞–º –Ω–∞–π–¥–µ—Ç –≤ .env –ø–æ –∏–º–µ–Ω–∏ KAFKA_BROKER
    kafka_broker: str 
    
    kafka_topic: str = "urls.fetched"

    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ===
    # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é. 
    # –ï—Å–ª–∏ –∑–∞—Ö–æ—á–µ—à—å —Å–º–µ–Ω–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤—å RSS_URL=... –≤ .env,
    # –Ω–µ –º–µ–Ω—è—è —ç—Ç–æ—Ç –∫–æ–¥.
    rss_url: str = "https://habr.com/ru/rss/articles/?fl=ru"
    poll_interval: int = 60

    # === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Pydantic ===
    # env_file=".env" –≥–æ–≤–æ—Ä–∏—Ç –∏—Å–∫–∞—Ç—å —Ñ–∞–π–ª –≤ –∫–æ—Ä–Ω–µ, –æ—Ç–∫—É–¥–∞ —Ç—ã –∑–∞–ø—É—Å–∫–∞–µ—à—å python
    # extra="ignore" –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ .env 
    # (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–∫–µ–Ω—ã –æ—Ç –¢–µ–ª–µ–≥—Ä–∞–º–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –ü–∞–±–ª–∏—à–µ—Ä—É, –Ω–æ –Ω–µ –Ω—É–∂–Ω—ã –ö–æ–ª–ª–µ–∫—Ç–æ—Ä—É)
    model_config = SettingsConfigDict(
        env_file=".env", 
        env_file_encoding="utf-8",
        extra="ignore"
    )

settings = Settings()

========================================
FILE: src/processor/main.py
========================================
import asyncio
import json
import logging
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
from config import settings
from parser import fetch_text_from_url
from summarizer import TextSummarizer 

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

async def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä (–Ω–∞—à AI-–º–æ–¥—É–ª—å, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 3 –≥–ª–∞–≤–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
    ai_model = TextSummarizer()
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Consumer (–ß–∏—Ç–∞–µ—Ç –∏–∑ urls.fetched)
    consumer = AIOKafkaConsumer(
        settings.topic_source,
        bootstrap_servers=settings.kafka_broker,
        group_id="content_processor_group", # –ì—Ä—É–ø–ø–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        value_deserializer=lambda x: json.loads(x.decode("utf-8"))
    )
    await consumer.start()

    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Producer (–ü–∏—à–µ—Ç –≤ content.ready)
    producer = AIOKafkaProducer(bootstrap_servers=settings.kafka_broker)
    await producer.start()

    logger.info("Processor started (with NLP AI). Waiting for messages...")

    try:
        # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        async for msg in consumer:
            data = msg.value
            url = data.get("link")
            title = data.get("title")
            
            logger.info(f"Processing: {title}")
            
            # 1. –°–∫–∞—á–∏–≤–∞–µ–º –∏ —á–∏—Å—Ç–∏–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ (async HTTP call)
            full_text = await fetch_text_from_url(url)
            
            if not full_text:
                logger.warning(f"Empty text for {url}, skipping...")
                continue

            # 2. === –ì–ï–ù–ï–†–ê–¶–ò–Ø SAMMARY (NLP) ===
            logger.info("Summarizing text...")
            try:
                summary = ai_model.summarize(full_text)
            except Exception as e:
                logger.error(f"Error summarizing text for {title}: {e}")
                # –ï—Å–ª–∏ AI —É–ø–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤
                summary = f"–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {full_text[:300]}..." 
            # =================================

            # 3. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            result_message = {
                "original_url": url,
                "title": title,
                "summary": summary, 
            }

            # 4. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–ø–∏–∫
            await producer.send_and_wait(
                settings.topic_result, 
                json.dumps(result_message).encode("utf-8")
            )
            logger.info(f"Successfully processed and sent: {title}")

    except asyncio.CancelledError:
        logger.info("Processor was cancelled.")
    finally:
        await consumer.stop()
        await producer.stop()
        logger.info("Processor stopped.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Service stopped manually")

========================================
FILE: src/processor/__init__.py
========================================


========================================
FILE: src/processor/config.py
========================================
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    kafka_broker: str
    topic_source: str = "urls.fetched"
    topic_result: str = "content.ready"
    
    # –ù–æ–≤—ã–π –∫–ª—é—á
    openrouter_api_key: str

    model_config = SettingsConfigDict(
        env_file=".env", 
        env_file_encoding="utf-8",
        extra="ignore"
    )

settings = Settings()

========================================
FILE: src/processor/parser.py
========================================
import trafilatura
from config import settings

async def fetch_text_from_url(url: str) -> str:
    try:
        # Trafilatura —É–º–µ–µ—Ç —Å–∞–º–∞ —Å–∫–∞—á–∏–≤–∞—Ç—å, –Ω–æ –ª—É—á—à–µ —Å–∫–∞—á–∞–µ–º –∏ –æ—Ç–¥–∞–¥–∏–º –µ–π —Å—Ç—Ä–æ–∫—É
        downloaded = trafilatura.fetch_url(url)
        
        if downloaded is None:
            return ""

        # include_comments=False —É–±–∏—Ä–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (—á–∞—Å—Ç—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –º—É—Å–æ—Ä–∞)
        # include_tables=False —É–±–∏—Ä–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã (–æ–Ω–∏ –ª–æ–º–∞—é—Ç —Å–∞–º–º–∞—Ä–∏)
        text = trafilatura.extract(
            downloaded, 
            include_comments=False, 
            include_tables=False,
            include_images=False,
            no_fallback=True
        )
        
        if not text:
            return ""
            
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        return text

    except Exception as e:
        print(f"Error extracting text from {url}: {e}")
        return ""

========================================
FILE: src/processor/summarizer.py
========================================
import httpx
import logging
import json
from config import settings

logger = logging.getLogger(__name__)

class TextSummarizer:
    def __init__(self):
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.model = "google/gemini-2.0-flash-lite-preview-02-05:free" # –ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
        self.headers = {
            "Authorization": f"Bearer {settings.openrouter_api_key}",
            "Content-Type": "application/json",
            # OpenRouter –ø—Ä–æ—Å–∏—Ç —ç—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–º–æ–∂–Ω–æ –ª—é–±—ã–µ)
            "HTTP-Referer": "https://github.com/marienbaum77/content-factory", 
            "X-Title": "Content Factory Bot"
        }

    async def summarize(self, text: str) -> str:
        if not text:
            return "–ù–µ—Ç —Ç–µ–∫—Å—Ç–∞."

        prompt = (
            f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É (summary) —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
            f"–ò–∑ –≤—ã–∂–∏–º–∫–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–Ω—è—Ç–Ω–æ, –æ —á–µ–º —ç—Ç–∞ —Å—Ç–∞—Ç—å—è. –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:\n\n{text[:15000]}"
        )

        payload = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }

        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    self.api_url, 
                    headers=self.headers, 
                    json=payload, 
                    timeout=45.0
                )

                if response.status_code != 200:
                    logger.error(f"OpenRouter Error: {response.text}")
                    return f"–û—à–∏–±–∫–∞ API: {response.status_code}"

                data = response.json()
                
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç OpenAI/OpenRouter
                try:
                    summary = data['choices'][0]['message']['content']
                    return summary
                except (KeyError, IndexError):
                    logger.error(f"Unexpected response format: {data}")
                    return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI."

            except Exception as e:
                logger.error(f"Critical AI Error: {e}")
                return "–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å AI."

========================================
FILE: src/__init__.py
========================================


========================================
FILE: src/storage/repository.py
========================================
from sqlalchemy import select
from sqlalchemy.exc import IntegrityError
from .db import async_session
from .models import ProcessedUrl

class UrlRepository:
    async def is_processed(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ —Å—Å—ã–ª–∫–∞ –≤ –±–∞–∑–µ"""
        async with async_session() as session:
            # SELECT 1 FROM processed_urls WHERE url = ...
            query = select(ProcessedUrl).where(ProcessedUrl.url == url)
            result = await session.execute(query)
            return result.scalar_one_or_none() is not None

    async def add(self, url: str, source: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫—É"""
        async with async_session() as session:
            try:
                new_entry = ProcessedUrl(url=url, source=source)
                session.add(new_entry)
                await session.commit()
            except IntegrityError:
                # –ï—Å–ª–∏ –≤–¥—Ä—É–≥ –≥–æ–Ω–∫–∞ –ø–æ—Ç–æ–∫–æ–≤ –∏ —Å—Å—ã–ª–∫–∞ —É–∂–µ –µ—Å—Ç—å
                await session.rollback()

========================================
FILE: src/storage/db.py
========================================
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from pydantic_settings import BaseSettings, SettingsConfigDict

class DbSettings(BaseSettings):
    db_host: str
    db_port: int
    db_user: str
    db_pass: str
    db_name: str
    
    # –í–û–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –¥–æ–±–∞–≤–ª—è–µ–º extra="ignore"
    model_config = SettingsConfigDict(
        env_file=".env", 
        env_file_encoding="utf-8",
        extra="ignore" 
    )

config = DbSettings()

DATABASE_URL = f"postgresql+asyncpg://{config.db_user}:{config.db_pass}@{config.db_host}:{config.db_port}/{config.db_name}"

engine = create_async_engine(DATABASE_URL, echo=False)

async_session = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False
)

Base = declarative_base()

async def init_db():
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

========================================
FILE: src/storage/models.py
========================================
from sqlalchemy import Column, String, Integer, DateTime, func
from .db import Base

class ProcessedUrl(Base):
    __tablename__ = "processed_urls"

    id = Column(Integer, primary_key=True, index=True)
    url = Column(String, unique=True, index=True) # unique=True - –∑–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–µ–π
    source = Column(String)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

========================================
FILE: src/storage/__init__.py
========================================
